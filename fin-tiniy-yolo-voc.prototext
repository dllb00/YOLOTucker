name: "Darkent2Caffe"
input: "data"
input_dim: 1
input_dim: 3
input_dim: 416
input_dim: 416

layer {
    bottom: "data"
    top: "conv1"
    name: "conv1"
    type: "Convolution"
    convolution_param {
        num_output: 16
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "conv1"
    top: "conv1_bn"
    name: "conv1_bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "conv1_bn"
    top: "conv1_scale"
    name: "conv1_scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "conv1_scale"
    top: "conv1_scale"
    name: "relu1"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "conv1_scale"
    top: "pool1"
    name: "pool1"
    type: "Pooling"
    pooling_param {
        kernel_size: 2
        stride: 2
        pool: MAX
    }
}
layer {
    bottom: "pool1"
    top: "conv2"
    name: "conv2"
    type: "Convolution"
    convolution_param {
        num_output: 32
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "conv2"
    top: "conv2_bn"
    name: "conv2_bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "conv2_bn"
    top: "conv2_scale"
    name: "conv2_scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "conv2_scale"
    top: "conv2_scale"
    name: "relu2"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "conv2_scale"
    top: "pool2"
    name: "pool2"
    type: "Pooling"
    pooling_param {
        kernel_size: 2
        stride: 2
        pool: MAX
    }
}
layer {
    bottom: "pool2"
    top: "conv3"
    name: "conv3"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "conv3"
    top: "conv3_bn"
    name: "conv3_bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "conv3_bn"
    top: "conv3_scale"
    name: "conv3_scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "conv3_scale"
    top: "conv3_scale"
    name: "relu3"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "conv3_scale"
    top: "pool3"
    name: "pool3"
    type: "Pooling"
    pooling_param {
        kernel_size: 2
        stride: 2
        pool: MAX
    }
}
layer {
    bottom: "pool3"
    top: "conv4"
    name: "conv4"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "conv4"
    top: "conv4_bn"
    name: "conv4_bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "conv4_bn"
    top: "conv4_scale"
    name: "conv4_scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "conv4_scale"
    top: "conv4_scale"
    name: "relu4"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "conv4_scale"
    top: "pool4"
    name: "pool4"
    type: "Pooling"
    pooling_param {
        kernel_size: 2
        stride: 2
        pool: MAX
    }
}
layer {
    bottom: "pool4"
    top: "conv5"
    name: "conv5"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "conv5"
    top: "conv5_bn"
    name: "conv5_bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "conv5_bn"
    top: "conv5_scale"
    name: "conv5_scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "conv5_scale"
    top: "conv5_scale"
    name: "relu5"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "conv5_scale"
    top: "pool5"
    name: "pool5"
    type: "Pooling"
    pooling_param {
        kernel_size: 2
        stride: 2
        pool: MAX
    }
}
layer {
    bottom: "pool5"
    top: "conv6"
    name: "conv6"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "conv6"
    top: "conv6_bn"
    name: "conv6_bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "conv6_bn"
    top: "conv6_scale"
    name: "conv6_scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "conv6_scale"
    top: "conv6_scale"
    name: "relu6"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "conv6_scale"
    top: "pool6"
    name: "pool6"
    type: "Pooling"
    pooling_param {
        kernel_size: 2
        stride: 1
        pool: MAX
    }
}
layer {
    bottom: "pool6"
    top: "conv7"
    name: "conv7"
    type: "Convolution"
    convolution_param {
        num_output: 1024
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "conv7"
    top: "conv7_bn"
    name: "conv7_bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "conv7_bn"
    top: "conv7_scale"
    name: "conv7_scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "conv7_scale"
    top: "conv7_scale"
    name: "relu7"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "conv7_scale"
    top: "conv8"
    name: "conv8"
    type: "Convolution"
    convolution_param {
        num_output: 1024
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "conv8"
    top: "conv8_bn"
    name: "conv8_bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "conv8_bn"
    top: "conv8_scale"
    name: "conv8_scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "conv8_scale"
    top: "conv8_scale"
    name: "relu8"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "conv8_scale"
    top: "conv9"
    name: "conv9"
    type: "Convolution"
    convolution_param {
        num_output: 125
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: true
    }
}